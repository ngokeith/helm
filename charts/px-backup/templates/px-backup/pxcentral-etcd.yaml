{{/* Setting defaults if they are omitted. */}}
{{- $etcdEnabled := .Values.pxbackup.enabled | default false }}
{{- $externalPersistentStorageEnabled := .Values.persistentStorage.enabled | default false }}
{{- $isOpenshiftCluster := .Capabilities.APIVersions.Has "apps.openshift.io/v1" -}}
{{- if eq $etcdEnabled true }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: pxc-backup-etcd-scripts
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: pxc-backup-etcd
  {{- include "px-backup.labels" . | nindent 4 }}
data:
  setup.sh: |-
    #!/bin/bash

    set -o errexit
    set -o pipefail
    set -o nounset

    # Debug section
    exec 3>&1
    exec 4>&2

    if [[ "${BITNAMI_DEBUG:-false}" = true ]]; then
        echo "==> Bash debug is on"
    else
        echo "==> Bash debug is off"
        exec 1>/dev/null
        exec 2>/dev/null
    fi

    # Constants
    HOSTNAME="$(hostname -s)"
    AUTH_OPTIONS=""
    export ETCDCTL_ENDPOINTS="pxc-backup-etcd-0.pxc-backup-etcd-headless:2380,pxc-backup-etcd-1.pxc-backup-etcd-headless:2380,pxc-backup-etcd-2.pxc-backup-etcd-headless:2380"
    export ROOT_PASSWORD="${ETCD_ROOT_PASSWORD:-}"
    if [[ -n "${ETCD_ROOT_PASSWORD:-}" ]]; then
      unset ETCD_ROOT_PASSWORD
    fi
    # Functions
    ## Store member id for later member replacement
    store_member_id() {
        while ! etcdctl $AUTH_OPTIONS member list; do sleep 1; done
        etcdctl $AUTH_OPTIONS member list | grep -w "$HOSTNAME" | awk '{ print $1}' | awk -F "," '{ print $1}' > "$ETCD_DATA_DIR/member_id"
        echo "==> Stored member id: $(cat ${ETCD_DATA_DIR}/member_id)" 1>&3 2>&4
        exit 0
    }
    ## Configure RBAC
    configure_rbac() {
        # When there's more than one replica, we can assume the 1st member
        # to be created is "pxc-backup-etcd-0" since a statefulset is used
        if [[ -n "${ROOT_PASSWORD:-}" ]] && [[ "$HOSTNAME" == "pxc-backup-etcd-0" ]]; then
            echo "==> Configuring RBAC authentication!" 1>&3 2>&4
            etcd &
            ETCD_PID=$!
            while ! etcdctl $AUTH_OPTIONS member list; do sleep 1; done
            echo "$ROOT_PASSWORD" | etcdctl $AUTH_OPTIONS user add root --interactive=false
            etcdctl $AUTH_OPTIONS auth enable
            kill "$ETCD_PID"
            sleep 5
        fi
    }
    ## Checks whether there was a disaster or not
    is_disastrous_failure() {
        local endpoints_array=(${ETCDCTL_ENDPOINTS//,/ })
        local active_endpoints=0
        local -r min_endpoints=$(((3 + 1)/2))

        for e in "${endpoints_array[@]}"; do
            if [[ "$e" != "$ETCD_ADVERTISE_CLIENT_URLS" ]] && (unset -v ETCDCTL_ENDPOINTS; etcdctl $AUTH_OPTIONS  endpoint health --endpoints="$e"); then
                active_endpoints=$((active_endpoints + 1))
            fi
        done
        if [[ -f "/snapshots/.disaster_recovery" ]]; then
            if [[ $active_endpoints -eq $((3 - 1)) ]]; then
                echo "==> I'm the last to recover from the disaster!" 1>&3 2>&4
                rm "/snapshots/.disaster_recovery" 1>&3 2>&4
            fi
            true
        else
            if [[ $active_endpoints -lt $min_endpoints ]]; then
                touch "/snapshots/.disaster_recovery" 1>&3 2>&4
                true
            else
                false
            fi
        fi
    }

    ## Check wether the member was succesfully removed from the cluster
    should_add_new_member() {
        return_value=0
        if (grep -E "^Member[[:space:]]+[a-z0-9]+\s+removed\s+from\s+cluster\s+[a-z0-9]+$" "$(dirname "$ETCD_DATA_DIR")/member_removal.log") || \
           ! ([[ -d "$ETCD_DATA_DIR/member/snap" ]] && [[ -f "$ETCD_DATA_DIR/member_id" ]]); then
            rm -rf $ETCD_DATA_DIR/* 1>&3 2>&4
        else
            return_value=1
        fi
        rm -f "$(dirname "$ETCD_DATA_DIR")/member_removal.log" 1>&3 2>&4
        return $return_value
    }

    if [[ ! -d "$ETCD_DATA_DIR" ]]; then
        echo "==> Creating data dir..." 1>&3 2>&4
        echo "==> There is no data at all. Initializing a new member of the cluster..." 1>&3 2>&4
        store_member_id & 1>&3 2>&4
        configure_rbac
    else
        echo "==> Detected data from previous deployments..." 1>&3 2>&4
        if [[ $(stat -c "%a" "$ETCD_DATA_DIR") != *700 ]]; then
            echo "==> Setting data directory permissions to 700 in a recursive way (required in etcd >=3.4.10)" 1>&3 2>&4
            chmod -R 700 $ETCD_DATA_DIR
        else
            echo "==> The data directory is already configured with the proper permissions" 1>&3 2>&4
        fi
        if [[ 3 -eq 1 ]]; then
            echo "==> Single node cluster detected!!" 1>&3 2>&4
        elif is_disastrous_failure; then
            echo "==> Cluster not responding!!" 1>&3 2>&4
            latest_snapshot_file="$(find /snapshots/ -maxdepth 1 -type f -name 'db-*' | sort | tail -n 1)"
            if [[ "${latest_snapshot_file}" != "" ]]; then
                echo "==> Restoring etcd cluster from snapshot..." 1>&3 2>&4

                rm -rf $ETCD_DATA_DIR 1>&3 2>&4
                etcdctl snapshot restore "${latest_snapshot_file}" \
                  --name $ETCD_NAME \
                  --data-dir $ETCD_DATA_DIR \
                  --initial-cluster $ETCD_INITIAL_CLUSTER \
                  --initial-cluster-token $ETCD_INITIAL_CLUSTER_TOKEN \
                  --initial-advertise-peer-urls $ETCD_INITIAL_ADVERTISE_PEER_URLS 1>&3 2>&4
                store_member_id & 1>&3 2>&4
            else
                echo "==> There was no snapshot to perform data recovery!!" 1>&3 2>&4
                exit 1
            fi
        elif should_add_new_member; then
            echo "==> Adding new member to existing cluster..." 1>&3 2>&4
            etcdctl $AUTH_OPTIONS member add "$HOSTNAME" --peer-urls="http://${HOSTNAME}.pxc-backup-etcd-headless:2380" | grep "^ETCD_" > "$ETCD_DATA_DIR/new_member_envs"
            sed -ie "s/^/export /" "$ETCD_DATA_DIR/new_member_envs"
            echo "==> Loading env vars of existing cluster..." 1>&3 2>&4
            source "$ETCD_DATA_DIR/new_member_envs" 1>&3 2>&4
            store_member_id & 1>&3 2>&4
        else
            echo "==> Updating member in existing cluster..." 1>&3 2>&4
            etcdctl $AUTH_OPTIONS member update "$(cat "$ETCD_DATA_DIR/member_id")" --peer-urls="http://${HOSTNAME}.pxc-backup-etcd-headless:2380" 1>&3 2>&4
        fi
    fi
    exec etcd 1>&3 2>&4
  prestop-hook.sh: |-
    #!/bin/bash

    set -o errexit
    set -o pipefail
    set -o nounset

    # Debug section
    exec 3>&1
    exec 4>&2

    if [[ "${BITNAMI_DEBUG:-false}" = true ]]; then
      echo "==> Bash debug is on"
    else
      echo "==> Bash debug is off"
      exec 1>/dev/null
      exec 2>/dev/null
    fi

    # Constants
    AUTH_OPTIONS=""
    export ETCDCTL_ENDPOINTS="pxc-backup-etcd-0.pxc-backup-etcd-headless:2380,pxc-backup-etcd-1.pxc-backup-etcd-headless:2380,pxc-backup-etcd-2.pxc-backup-etcd-headless:2380"

    etcdctl $AUTH_OPTIONS member remove --debug=true "$(cat "$ETCD_DATA_DIR/member_id")" > "$(dirname "$ETCD_DATA_DIR")/member_removal.log" 2>&1
  probes.sh: |-
    #!/bin/bash

    set -o errexit
    set -o pipefail
    set -o nounset

    # Debug section
    exec 3>&1
    exec 4>&2

    if [[ "${BITNAMI_DEBUG:-false}" = true ]]; then
        echo "==> Bash debug is on"
    else
        echo "==> Bash debug is off"
        exec 1>/dev/null
        exec 2>/dev/null
    fi

    # Constants
    AUTH_OPTIONS=""

    echo "==> [DEBUG] Probing etcd cluster"
    echo "==> [DEBUG] Probe command: \"etcdctl $AUTH_OPTIONS endpoint health\""
    etcdctl $AUTH_OPTIONS endpoint health
  save-snapshot.sh: |-
    #!/bin/bash

    set -o errexit
    set -o pipefail
    set -o nounset

    # Debug section
    exec 3>&1
    exec 4>&2

    if [[ "${BITNAMI_SNAPSHOT_DEBUG:-false}" = true ]] || [[ "${BITNAMI_DEBUG:-false}" = true ]]; then
        echo "==> Bash debug is on"
        set -x
    else
        echo "==> Bash debug is off"
        exec 1>/dev/null
        exec 2>/dev/null
    fi

    # Constants
    AUTH_OPTIONS=""
    export ETCDCTL_ENDPOINTS="pxc-backup-etcd-0.pxc-backup-etcd-headless:2380,pxc-backup-etcd-1.pxc-backup-etcd-headless:2380,pxc-backup-etcd-2.pxc-backup-etcd-headless:2380"

    mkdir -p "/snapshots" 1>&3 2>&4

    read -r -a endpoints <<< "$(tr ',;' ' ' <<< "$ETCDCTL_ENDPOINTS")"
    for endp in "${endpoints[@]}"; do
        echo "Using endpoint $endp" 1>&3 2>&4
        if (unset -v ETCDCTL_ENDPOINTS; etcdctl $AUTH_OPTIONS endpoint health --endpoints=${endp}); then
            echo "Snapshotting the keyspace..." 1>&3 2>&4
            current_time="$(date -u "+%Y-%m-%d_%H-%M")"
            unset -v ETCDCTL_ENDPOINTS; etcdctl $AUTH_OPTIONS snapshot save "/snapshots/db-${current_time}" --endpoints=${endp} 1>&3 2>&4
            find /snapshots/ -maxdepth 1 -type f -name 'db-*' \! -name "db-${current_time}" \
                | sort -r \
                | tail -n+$((1 + 1)) \
                | xargs rm -f 1>&3 2>&4
            exit 0
        else
            echo "Warning - etcd endpoint ${ETCDCTL_ENDPOINTS} not healthy" 1>&3 2>&4
            echo "Trying another endpoint." 1>&3 2>&4
        fi
    done

    # exit with error if all endpoints are bad
    echo "Error - all etcd endpoints are unhealthy!" 1>&3 2>&4
    exit 1
---
{{- if eq $externalPersistentStorageEnabled true }}
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pxc-backup-etcd-snapshotter
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: pxc-backup-etcd
{{- include "px-backup.labels" . | nindent 4 }}
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: "10Gi"
  storageClassName: {{ .Values.persistentStorage.storageClassName }}
{{- end }}
---
apiVersion: v1
kind: Service
metadata:
  name: pxc-backup-etcd-headless
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: pxc-backup-etcd
{{- include "px-backup.labels" . | nindent 4 }}
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/component: pxc-backup-etcd
    app.kubernetes.io/name: {{.Release.Name }}
    app.kubernetes.io/instance: {{.Release.Name }}
---
apiVersion: v1
kind: Service
metadata:
  name: pxc-backup-etcd
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: pxc-backup-etcd
{{- include "px-backup.labels" . | nindent 4 }}
spec:
  type: ClusterIP
  ports:
    - name: client
      port: 2379
      targetPort: client
      nodePort: null
    - name: peer
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/component: pxc-backup-etcd
    app.kubernetes.io/name: {{.Release.Name }}
    app.kubernetes.io/instance: {{.Release.Name }}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: pxc-backup-etcd
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: pxc-backup-etcd
{{- include "px-backup.labels" . | nindent 4 }}
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: {{.Release.Name }}
      app.kubernetes.io/instance: {{.Release.Name }}
      app.kubernetes.io/managed-by: {{.Release.Service }}
      app.kubernetes.io/component: pxc-backup-etcd
  serviceName: pxc-backup-etcd-headless
  podManagementPolicy: Parallel
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{.Release.Name }}
        app.kubernetes.io/instance: {{.Release.Name }}
        app.kubernetes.io/managed-by: {{.Release.Service }}
        app.kubernetes.io/component: pxc-backup-etcd
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: pxbackup/enabled
                operator: NotIn
                values:
                - "false"
      {{- if eq .Values.storkRequired true }}
      schedulerName: stork
      {{- end }}
      {{- if .Values.images.pullSecrets }}
      imagePullSecrets:
        {{- range $sec := .Values.images.pullSecrets }}
        - name: {{ $sec | quote }}
        {{- end }}
      {{- end }}
      serviceAccountName: px-backup-account
      {{- if $isOpenshiftCluster}}
      {{- else }}
      securityContext:
{{ toYaml .Values.securityContext | indent 8 }}
      {{- end }}
      containers:
        - name: etcd
          image: {{ printf "%s/%s/%s:%s" .Values.images.etcdImage.registry .Values.images.etcdImage.repo .Values.images.etcdImage.imageName .Values.images.etcdImage.tag }}
          imagePullPolicy: {{ .Values.images.pullPolicy }}
          command:
            - /scripts/setup.sh
          lifecycle:
            preStop:
              exec:
                command:
                  - /scripts/prestop-hook.sh
          resources:
            limits: {}
            requests: {}
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: /bitnami/etcd/data
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).pxc-backup-etcd-headless:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).pxc-backup-etcd-headless:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_INITIAL_CLUSTER_TOKEN
              value: "pxc-backup-etcd-cluster"
            - name: ETCD_INITIAL_CLUSTER_STATE
              value: "new"
            - name: ETCD_INITIAL_CLUSTER
              value: "pxc-backup-etcd-0=http://pxc-backup-etcd-0.pxc-backup-etcd-headless:2380,pxc-backup-etcd-1=http://pxc-backup-etcd-1.pxc-backup-etcd-headless:2380,pxc-backup-etcd-2=http://pxc-backup-etcd-2.pxc-backup-etcd-headless:2380"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTO_COMPACTION_RETENTION
              value: "10m"
            - name: ETCD_AUTO_COMPACTION_MODE
              value: "periodic"
            - name: ETCD_QUOTA_BACKEND_BYTES
              value: "8589934592"
          ports:
            - name: client
              containerPort: 2379
            - name: peer
              containerPort: 2380
          livenessProbe:
            exec:
              command:
                - /scripts/probes.sh
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /scripts/probes.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: scripts
              mountPath: /scripts/prestop-hook.sh
              subPath: prestop-hook.sh
            - name: scripts
              mountPath: /scripts/probes.sh
              subPath: probes.sh
            - name: pxc-etcd-data
              mountPath: /bitnami/etcd
            - name: snapshot-volume
              mountPath: /snapshots
      volumes:
        - name: scripts
          configMap:
            name: pxc-backup-etcd-scripts
            defaultMode: 0755
        {{- if eq $externalPersistentStorageEnabled false }}
        - name: pxc-etcd-data
          emptyDir: {}
        - name: snapshot-volume
          emptyDir: {}
        {{- else }}
        - name: snapshot-volume
          persistentVolumeClaim:
            claimName: pxc-backup-etcd-snapshotter
        {{- end }}
  {{- if eq $externalPersistentStorageEnabled true }}
  volumeClaimTemplates:
    - metadata:
        name: pxc-etcd-data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "64Gi"
        {{- if .Values.persistentStorage.storageClassName }}
        storageClassName: {{ .Values.persistentStorage.storageClassName }}
        {{- end }}
  {{- end }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: pxc-backup-etcd-snapshotter
  labels:
    app.kubernetes.io/component: snapshotter
{{- include "px-backup.labels" . | nindent 4 }}
spec:
  concurrencyPolicy: Forbid
  schedule: "*/30 * * * *"
  successfulJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: etcd
            app.kubernetes.io/instance: pxc-backup-etcd
            app.kubernetes.io/component: snapshotter
        spec:
          restartPolicy: OnFailure
          {{- if $isOpenshiftCluster}}
          {{- else }}
          securityContext:
{{ toYaml .Values.securityContext | indent 12 }}
          {{- end }}
          {{- if .Values.images.pullSecrets }}
          imagePullSecrets:
            {{- range $sec := .Values.images.pullSecrets }}
            - name: {{ $sec | quote }}
            {{- end }}
          {{- end }}
          containers:
            - name: etcd-snapshotter
              image: {{ printf "%s/%s/%s:%s" .Values.images.etcdImage.registry .Values.images.etcdImage.repo .Values.images.etcdImage.imageName .Values.images.etcdImage.tag }}
              imagePullPolicy: {{ .Values.images.pullPolicy }}
              command:
                - /scripts/save-snapshot.sh
              env:
              - name: BITNAMI_SNAPSHOT_DEBUG
                value: "true"
              - name: BITNAMI_DEBUG
                value: "false"
              - name: ETCDCTL_API
                value: "3"
              volumeMounts:
                - name: scripts
                  mountPath: /scripts/save-snapshot.sh
                  subPath: save-snapshot.sh
                - name: snapshot-volume
                  mountPath: /snapshots
          volumes:
            - name: scripts
              configMap:
                name: pxc-backup-etcd-scripts
                defaultMode: 0755
            {{- if eq $externalPersistentStorageEnabled false }}
            - name: snapshot-volume
              emptyDir: {}
            {{- else }}
            - name: snapshot-volume
              persistentVolumeClaim:
                claimName: pxc-backup-etcd-snapshotter
            {{- end }}
{{- end -}}